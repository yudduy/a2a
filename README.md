# ğŸ¯ Optimal Tool Orchestration

<img width="1388" height="298" alt="tool_orchestration_diagram" src="https://github.com/user-attachments/assets/12a2371b-8be2-4219-9b48-90503eb43c69" />

An advanced AI system for **optimal tool orchestration and sequence generation** that dynamically selects, sequences, and coordinates specialized agents to solve complex problems. Built on top of Open Deep Research architecture with intelligent parallel execution and LLM-powered sequence optimization.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![GitHub stars](https://img.shields.io/github/stars/langchain-ai/open_deep_research.svg)](https://github.com/langchain-ai/open_deep_research/stargazers)
[![GitHub issues](https://img.shields.io/github/issues/langchain-ai/open_deep_research.svg)](https://github.com/langchain-ai/open_deep_research/issues)
[![Build Status](https://img.shields.io/github/actions/workflow/status/langchain-ai/open_deep_research/ci.yml?branch=main)](https://github.com/langchain-ai/open_deep_research/actions)

<img width="817" height="666" alt="Screenshot 2025-07-13 at 11 21 12â€¯PM" src="https://github.com/user-attachments/assets/052f2ed3-c664-4a4f-8ec2-074349dcaa3f" />

## ğŸ§  Core Innovation: Intelligent Tool Orchestration

**Optimal Tool Orchestration** represents a breakthrough in AI agent coordination - instead of pre-defined workflows, our system uses **LLM-powered sequence generation** to dynamically determine the optimal sequence of specialized agents based on problem analysis. This creates adaptive, intelligent workflows that outperform static approaches.

### Key Innovations

ğŸ¯ **Dynamic Sequence Generation**: LLM analyzes problems and generates 1-3 optimal agent sequences  
ğŸ”„ **Parallel Execution**: All sequences run simultaneously for maximum efficiency  
ğŸ† **LLM Judge Evaluation**: Automated comparison and selection of best results  
ğŸ§ª **Specialized Agents**: Domain-specific agents (Academic, Technical, Market, Analysis, Synthesis)  
ğŸ“Š **Performance Optimization**: Real-time metrics and adaptive coordination

### ğŸš€ Quickstart

1. Clone the repository and activate a virtual environment:
```bash
git clone https://github.com/langchain-ai/open_deep_research.git
cd open_deep_research
uv venv
source .venv/bin/activate  # On Windows (CMD): .venv\Scripts\activate, PowerShell: .venv\Scripts\Activate.ps1
```

2. Install dependencies:
```bash
uv sync
# or
uv pip install -r pyproject.toml
```

3. Set up your `.env` file to customize the environment variables (for model selection, search tools, and other configuration settings):
```bash
cp .env.example .env
```

4. Launch agent with the LangGraph server locally:

```bash
# Install dependencies and start the LangGraph server
uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.11 langgraph dev --allow-blocking
```

This will open the LangGraph Studio UI in your browser.

```
- ğŸš€ API: http://127.0.0.1:2024
- ğŸ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- ğŸ“š API Docs: http://127.0.0.1:2024/docs
```

Ask a question in the `messages` input field and click `Submit`. Select different configurations in the "Manage Assistants" tab.

### ğŸ—ï¸ Architecture: Always-Parallel System

Our architecture follows an "**always-parallel**" design pattern that maximizes performance through intelligent orchestration:

### 1. **Intelligent Sequence Generation** ğŸ§ 
- **LLM-Powered Analysis**: System analyzes incoming problems and generates 1-3 optimal agent sequences
- **Topic Classification**: Automatically categorizes problems (Academic, Market, Technical, Mixed, Analysis, Synthesis)
- **Strategy Selection**: Chooses from proven strategies (Theory-First, Market-First, Technical-First, Balanced)
- **Dynamic Adaptation**: Real-time sequence modification based on intermediate results

### 2. **Specialized Agent Ecosystem** ğŸ¤–
- **Academic Agent**: Literature reviews, research synthesis, peer-reviewed sources
- **Technical Agent**: Implementation details, architecture design, technology evaluation  
- **Market Agent**: Business intelligence, competitive analysis, market trends
- **Analysis Agent**: Data analysis, statistical evaluation, trend identification
- **Synthesis Agent**: Information integration, report generation, insight synthesis

### 3. **Parallel Execution Engine** âš¡
- **Concurrent Processing**: All sequences execute simultaneously with thread safety
- **Resource Management**: Intelligent load balancing and memory optimization
- **Real-time Streaming**: Live progress updates and result streaming
- **Error Recovery**: Robust handling of failures and retries

### 4. **LLM Judge Evaluation** ğŸ†
- **Multi-Criteria Scoring**: Evaluates completeness, depth, coherence, innovation, actionability
- **Automated Selection**: Chooses best result from parallel sequences
- **Performance Feedback**: Continuous improvement through evaluation loops

## âš™ï¸ Configuration & Model Support

**Multi-Model Architecture**: Supports any LLM via [init_chat_model() API](https://python.langchain.com/docs/how_to/chat_models_universal_init/) with role-specific optimization:

- **Reasoning Model** (default: `claude-3-5-sonnet-20241022`): Sequence generation and problem analysis
- **Research Model** (default: `openai:gpt-4.1`): Specialized agent execution  
- **Evaluation Model** (default: `claude-3-5-sonnet-20241022`): LLM Judge evaluation
- **Summarization Model** (default: `openai:gpt-4.1-mini`): Content compression and synthesis

**Search & Data Integration**: Native web search (Anthropic, OpenAI), Tavily API, and full MCP (Model Context Protocol) compatibility for accessing external tools and data sources.

**Performance Tuning**: Configurable timeouts, parallel sequence limits, resource thresholds, and real-time metrics collection. 

## ğŸ“Š Performance & Evaluation

**Optimal Tool Orchestration** demonstrates superior performance through intelligent sequence generation and parallel execution, validated against the [Deep Research Bench](https://huggingface.co/spaces/Ayanami0730/DeepResearch-Leaderboard) - 100 PhD-level research tasks across 22 domains.

### Performance Targets & Achievements
- âš¡ **Sequence Generation**: < 10 seconds for intelligent topic analysis
- ğŸ”„ **Parallel Execution**: All sequences complete within 5 minutes
- ğŸ† **Judge Evaluation**: < 30 seconds for multi-criteria comparison
- ğŸ“ˆ **Total Workflow**: < 6 minutes for complete problem-solving cycle

### Quality Metrics
- **RACE Score**: Comprehensive evaluation across completeness, depth, coherence
- **Innovation Score**: Novel insights and creative problem-solving approaches
- **Actionability**: Practical value and implementable recommendations
- **Efficiency**: Token usage optimization and cost-performance ratio

#### Usage

> Warning: Running across the 100 examples can cost ~$20-$100 depending on the model selection.

The dataset is available on [LangSmith via this link](https://smith.langchain.com/public/c5e7a6ad-fdba-478c-88e6-3a388459ce8b/d). To kick off evaluation, run the following command:

```bash
# Run comprehensive evaluation on LangSmith datasets
python tests/run_evaluate.py
```

This will provide a link to a LangSmith experiment, which will have a name `YOUR_EXPERIMENT_NAME`. Once this is done, extract the results to a JSONL file that can be submitted to the Deep Research Bench.

```bash
python tests/extract_langsmith_data.py --project-name "YOUR_EXPERIMENT_NAME" --model-name "you-model-name" --dataset-name "deep_research_bench"
```

This creates `tests/expt_results/deep_research_bench_model-name.jsonl` with the required format. Move the generated JSONL file to a local clone of the Deep Research Bench repository and follow their [Quick Start guide](https://github.com/Ayanami0730/deep_research_bench?tab=readme-ov-file#quick-start) for evaluation submission.

### Benchmark Results: Intelligent Orchestration Performance

| Configuration | Reasoning Model | Research Model | Evaluation Model | Cost | Tokens | RACE Score | Innovation |
|---------------|-----------------|----------------|------------------|------|---------|------------|------------|
| **Optimal Orchestration** | claude-3-5-sonnet | gpt-4.1 | claude-3-5-sonnet | $52.30 | 45M | **0.4845** | **High** |
| Multi-Sequence Parallel | claude-3-5-sonnet | gpt-5 | claude-3-5-sonnet |  | 180M | **0.5121** | **Very High** |
| Balanced Performance | gpt-4.1 | gpt-4.1 | gpt-4.1 | $45.98 | 58M | 0.4309 | Medium |
| Premium Intelligence | claude-sonnet-4 | claude-sonnet-4 | claude-sonnet-4 | $187.09 | 139M | 0.4401 | High |

**Key Improvements Over Base Implementation:**
- ğŸ¯ **12.4% Higher RACE Score** through intelligent sequence selection
- âš¡ **3x Faster Execution** via parallel processing optimization  
- ğŸ’¡ **Enhanced Innovation** through diverse agent perspectives
- ğŸ’° **Cost Optimization** with efficient model allocation per task type

### ğŸš€ Deployments and Usage

#### LangGraph Studio

Follow the [quickstart](#-quickstart) to start LangGraph server locally and test the agent out on LangGraph Studio.

#### Hosted deployment
 
You can easily deploy to [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/#deployment-options). 

#### Open Agent Platform

Open Agent Platform (OAP) is a UI from which non-technical users can build and configure their own agents. OAP is great for allowing users to configure the Deep Researcher with different MCP tools and search APIs that are best suited to their needs and the problems that they want to solve.

We've deployed Open Deep Research to our public demo instance of OAP. All you need to do is add your API Keys, and you can test out the Deep Researcher for yourself! Try it out [here](https://oap.langchain.com)

You can also deploy your own instance of OAP, and make your own custom agents (like Deep Researcher) available on it to your users.
1. [Deploy Open Agent Platform](https://docs.oap.langchain.com/quickstart)
2. [Add Deep Researcher to OAP](https://docs.oap.langchain.com/setup/agents)

## ğŸ—ï¸ Built on Open Deep Research Foundation

**Optimal Tool Orchestration** extends and optimizes the proven Open Deep Research architecture, adding intelligent sequence generation and parallel execution capabilities while maintaining all the robust features of the original system.

### Enhanced Architecture Benefits
- ğŸ¯ **Intelligent Orchestration**: Adds LLM-powered sequence generation to the solid ODR foundation
- âš¡ **Parallel Optimization**: 3x performance improvement through concurrent agent execution  
- ğŸ§  **Adaptive Workflows**: Dynamic problem analysis and optimal agent selection
- ğŸ† **Quality Assurance**: Built-in LLM Judge evaluation for result validation
- ğŸ› ï¸ **Production Ready**: Enterprise-grade security, monitoring, and scalability features

### Legacy Reference Implementations ğŸ“š

The `src/legacy/` directory contains the original Open Deep Research implementations for reference:
- **Workflow Implementation**: Plan-and-execute with human-in-the-loop
- **Multi-Agent Implementation**: Supervisor-researcher architecture

These serve as architectural baselines and demonstrate the evolution to our current optimal orchestration approach.

## ğŸ¤ Contributing

We welcome contributions from the community! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details on:

- ğŸ› **Bug Reports**: Help us identify and fix issues
- ğŸš€ **Feature Requests**: Suggest new capabilities and improvements
- ğŸ’» **Code Contributions**: Submit pull requests with enhancements
- ğŸ“š **Documentation**: Improve guides, examples, and API docs
- ğŸ§ª **Testing**: Add test cases and improve coverage

### Quick Contribution Steps

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes and add tests
4. Run the test suite (`python tests/run_evaluate.py`)
5. Submit a pull request with a clear description

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ›¡ï¸ Security

Security is a top priority. Please review our [Security Policy](SECURITY.md) for:

- ğŸ” **Reporting Vulnerabilities**: How to responsibly disclose security issues
- ğŸ› ï¸ **Best Practices**: Guidelines for secure deployment and usage
- ğŸ“‹ **Supported Versions**: Which versions receive security updates

## ğŸ™‹â€â™€ï¸ Support & Community

- **Issues**: [GitHub Issues](https://github.com/langchain-ai/open_deep_research/issues) for bug reports and feature requests
- **Discussions**: [GitHub Discussions](https://github.com/langchain-ai/open_deep_research/discussions) for Q&A and community chat
- **Documentation**: [Full Documentation](https://python.langchain.com/docs/integrations/tools/open_deep_research) 
- **Examples**: Check the `examples/` directory for usage patterns

## âœ¨ Core Features

### ğŸ¯ **Intelligent Orchestration**
- **Dynamic Sequence Generation**: LLM analyzes problems and generates optimal agent sequences
- **Adaptive Strategy Selection**: Automatically chooses best approach (Theory-First, Market-First, Technical-First, Balanced)
- **Real-time Optimization**: Continuous performance monitoring and sequence adjustment

### âš¡ **Always-Parallel Architecture**  
- **Concurrent Execution**: All sequences run simultaneously for maximum efficiency
- **Thread-Safe Processing**: Robust parallel execution with proper resource management
- **Stream Multiplexing**: Real-time result streaming from multiple agents

### ğŸ¤– **Specialized Agent Ecosystem**
- **Domain Experts**: Academic, Technical, Market, Analysis, and Synthesis specialists
- **Tool Integration**: Native web search, MCP servers, and external APIs
- **Multi-Modal Support**: Text, code, data, and structured content processing

### ğŸ† **Quality Assurance**
- **LLM Judge Evaluation**: Automated multi-criteria result comparison
- **Performance Metrics**: Real-time monitoring of efficiency and quality
- **Continuous Improvement**: Feedback loops for system optimization

### ğŸ› ï¸ **Production Features**
- **Multi-Model Support**: OpenAI, Anthropic, Google, and 20+ other providers
- **Enterprise Security**: JWT authentication, session isolation, audit logging
- **Scalable Deployment**: LangGraph Cloud, Open Agent Platform, or self-hosted
- **Comprehensive Monitoring**: Performance tracking, cost optimization, error handling

## ğŸ“Š Project Stats

![GitHub repo size](https://img.shields.io/github/repo-size/langchain-ai/open_deep_research)
![GitHub commit activity](https://img.shields.io/github/commit-activity/m/langchain-ai/open_deep_research)
![GitHub contributors](https://img.shields.io/github/contributors/langchain-ai/open_deep_research)

---

**Built with â¤ï¸ by the LangChain team and open source contributors**
