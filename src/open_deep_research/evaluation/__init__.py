"""Evaluation module for sequential multi-agent research reports.

This module provides LLM-based evaluation capabilities for comparing research reports
generated by different agent sequences. It includes comprehensive scoring across multiple
criteria and comparative analysis to determine the most effective sequence strategies.
"""

from .llm_judge import LLMJudge
from .models import (
    EvaluationCriteria,
    ReportEvaluation, 
    ComparativeAnalysis,
    EvaluationResult,
    SequenceComparison
)
from .prompts import EvaluationPrompts

__all__ = [
    "LLMJudge",
    "EvaluationCriteria",
    "ReportEvaluation",
    "ComparativeAnalysis", 
    "EvaluationResult",
    "SequenceComparison",
    "EvaluationPrompts"
]